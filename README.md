# TimeFrequencySpectrogramRecognization

12/16

利用深度模型来对时频图所代表的的语音进行分类识别
### 数据输入

我们的输入来自于对24个指令不同人的录音，之后通过项目 https://github.com/saturn-lab/audioPlot 生成时频谱图作为我们的原始数据进行分类。其中有约2300条数据，取10%作为验证集。
### 数据清洗及增强

在训练的过程中发现有些图片中不包含有效数据，限制了准确率的上升，并容易导致过拟合现象的出现。因此将这些图片删除后继续进行训练

##### 数据增强
* 添加白噪声
* 高斯模糊

由于时频图是已经过处理的数据，一些常用的CV增强方式都无法使用，故只选用了这两种较为普遍的方式。更好地处理方式我认为应该是直接向音频中添加噪声。

### 模型架构
1. 我们的输入数据维度为(513,800,3)，其中513为时间坐标，800为频率坐标。
2. 由于此图片较大，可能需要较深的CNN，先利用3层ResBlock来主要提取同一时刻频率方向上的特征。具体的参数见程序
3. 之后在时间维度上利用双向GRU进行编码，得到各个时刻的特征
4. 之后对一张图片上所有时间戳上的特征运用注意力机制得到各个时刻的权重，并将其按权重相加得到代表整张图的特征
5. 之后再利用两层全连接层，中间隐藏层维度约为指令语音中字（音节）的个数，最后经过softmax层输出

### 模型训练

令优化器为Adam,学习率一开始设为1e-4,进行训练。
##### 训练结果
* 运行500个epoch以后出现了过拟合现象，其中训练accuracy达到80%,验证集准确率只有约70%，后来发现主要原因为数据中出现了错误。
* 在此基础上删去错误图片，再过200个epoch出现过拟合,此时最好情况，训练集准确率96%，验证集准确率92%
* 添加白噪声作为数据增强，并减小学习率至1e-5,经过80个epoch,此时最好情况，训练集准确率97%，验证集准确率99.6%
* 额外添加高斯模糊作为数据增强，经过40个epoch，此时最好情况，训练集准确率98.15%，验证集准确率100%

这样的训练结果一部分原因在模型的表达能力足够强，另一方面我认为图片数目仍旧较小，其鲁棒性仍待进一步验证。

12/23

之前所得到的结果有过拟合的嫌疑,所以之后我进行了7-fold交叉验证，每个进行120个lr=1e-4的epoch以及40个lr=1e-5的epoch
得到结果见7-fold-crossvalidation.bmp
其中准确率在57.59%-78.51%之间
推测其原因在于网络过深以及数据量过小。
